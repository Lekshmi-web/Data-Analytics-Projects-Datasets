{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aba017cf-d849-4b7d-b5d5-c16b625a1d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Unclean HR dataset generated successfully with 10500 rows!\n",
      "üìÇ Saved as: Assignment2_HR_DT.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Constants\n",
    "num_rows = 10000\n",
    "departments = [\"HR\", \"Finance\", \"Engineering\", \"Marketing\", \"Sales\", \"Admin\", \"finance\", \"hr\", \"Engg\"]\n",
    "positions = [\"Manager\", \"Analyst\", \"Engineer\", \"Executive\", \"Clerk\", \"Director\", \"Intern\", \"engineer\", \"\"]\n",
    "cities = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"San Jose\", \"newyork\", \"LA\", \"chicago\"]\n",
    "\n",
    "# Generate messy HR data\n",
    "data = {\n",
    "    \"Employee_ID\": [f\"EMP{1000+i}\" for i in range(num_rows)],\n",
    "    \"Employee_Name\": [\n",
    "        str(random.choice([\"John\", \"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\", \"\"])) +\n",
    "        str(random.choice([\" Smith\", \" Johnson\", \" Brown\", \" \", \"\", None]))\n",
    "        for _ in range(num_rows)\n",
    "    ],\n",
    "    \"Gender\": [random.choice([\"Male\", \"Female\", \"M\", \"F\", \"male\", \"female\", \"\", None]) for _ in range(num_rows)],\n",
    "    \"Department\": [random.choice(departments) for _ in range(num_rows)],\n",
    "    \"Position\": [random.choice(positions) for _ in range(num_rows)],\n",
    "    \"Salary\": [\n",
    "        random.choice([round(random.uniform(25000, 200000), 2), \"N/A\", None, \"\", \"twenty thousand\"])\n",
    "        for _ in range(num_rows)\n",
    "    ],\n",
    "    \"Join_Date\": [\n",
    "        (datetime(2010, 1, 1) + timedelta(days=random.randint(0, 5000))).strftime(\"%Y-%m-%d\")\n",
    "        if random.random() > 0.05 \n",
    "        else random.choice([\"2015-15-01\", \"30/02/2017\", \"\", None])\n",
    "        for _ in range(num_rows)\n",
    "    ],\n",
    "    \"City\": [random.choice(cities) for _ in range(num_rows)],\n",
    "    \"Email\": [\n",
    "        random.choice([\n",
    "            \"john@gmail.com\", \"alice@\", \"bob@yahoo\", \"evehotmail.com\", \"charlie@gmail.com\", \"\",\n",
    "            None, \"diana@@gmail.com\"\n",
    "        ])\n",
    "        for _ in range(num_rows)\n",
    "    ],\n",
    "    \"Age\": [\n",
    "        random.choice([random.randint(20, 65), \"\", None, \"twenty five\", 120, -5])\n",
    "        for _ in range(num_rows)\n",
    "    ],\n",
    "    \"Performance_Score\": [\n",
    "        random.choice([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"Excellent\", \"Good\", \"average\", None, \"\"])\n",
    "        for _ in range(num_rows)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce duplicates (5%)\n",
    "duplicates = df.sample(frac=0.05, random_state=42)\n",
    "df_unclean = pd.concat([df, duplicates], ignore_index=True)\n",
    "\n",
    "# Shuffle rows\n",
    "df_unclean = df_unclean.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save to CSV\n",
    "file_path = \"Assignment2_HR_DT.csv\"\n",
    "df_unclean.to_csv(file_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Unclean HR dataset generated successfully with {len(df_unclean)} rows!\")\n",
    "print(f\"üìÇ Saved as: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50812fd6-128f-4ecf-bb8a-a0e650e5886a",
   "metadata": {},
   "source": [
    "Dataset Description:\n",
    "This is a synthetic unclean HR dataset with 10,000+ records, designed for data analytics, testing, and cleaning exercises. It includes realistic HR attributes like Employee ID, Name, Gender, Department, Position, Salary, Join Date, City, Email, Age, and Performance Score.\n",
    "\n",
    "The dataset intentionally contains messy and inconsistent data, including:\n",
    "\n",
    "Missing values and nulls\n",
    "\n",
    "Typos and inconsistent formatting (e.g., ‚ÄúHR‚Äù vs ‚Äúhr‚Äù, ‚ÄúEngineer‚Äù vs ‚Äúengineer‚Äù)\n",
    "\n",
    "Invalid or malformed entries (e.g., negative ages, invalid dates, wrong emails)\n",
    "\n",
    "Mixed data types (strings in numeric fields, etc.)\n",
    "\n",
    "Duplicates (~5% of the data)\n",
    "\n",
    "This makes it perfect for practicing data cleaning, preprocessing, and validation techniques in Python or any data analytics workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e239bbd-2837-430c-babe-bd3a5f4c7a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
